# Benchmark Harness Test Run — 2026-02-09

## Purpose
End-to-end test of the DaveChess benchmark CLI harness with three coding agents.
All opponents are MCTSLite placeholders (no trained neural network). Results are
for harness validation only — not real benchmark scores.

## Environment
- Game rules: DaveChess v2 (chess-style capture, pawn-like Warriors, promotion, no Power nodes)
- Opponents: MCTSLite (random rollouts, no NN)
- Calibration: Old v1 calibration.json (placeholder)
- GM games: 5 placeholder files (20 games, 200-sim MCTSLite vs random)
- Token budget: 500,000 per agent
- Baseline games: 10 (default)
- Eval games: min 5, max 15

## Agent Results

### Claude Code (claude-opus-4-6)
| Metric | Value |
|--------|-------|
| Completed | Yes |
| Baseline ELO | 969.1 |
| Final ELO | 943.9 |
| ELO Gain | -25.2 |
| Baseline Games | 5 / 10 |
| Eval Games | 1 |
| Tokens Used | 895,700 |
| CLI Calls | 357 |
| Token/Move | ~2,500 |
| Wall Time | ~16 min |
| Agent Cost | $0.80 |

**Approach**: Wrote a heuristic move-scoring function (captures > bombards >
gold node control > forward movement > piece development). Switched from
subprocess-based CLI calls to direct Python imports for speed. Hit Git Bash
path bug, fixed it autonomously.

**Baseline**: 4 draws, 1 win. Over-reported tokens (2000 prompt + 500 completion
per move) burned through budget during baseline, triggering early transition.

**Issues found**:
- Practice game "No legal moves" error on game_001
- GM game study returned 5 games but content appeared empty in output
- Token self-reporting inflated budget (2500/move is too high)

---

### Codex CLI v0.87.0 (GPT model via OpenAI)
| Metric | Value |
|--------|-------|
| Completed | Yes |
| Baseline ELO | 692.3 |
| Final ELO | 1105.9 |
| ELO Gain | +413.6 |
| Baseline Games | 10 / 10 |
| Eval Games | 1 |
| Tokens Used | 674,200 |
| CLI Calls | 2,246 |
| Token/Move | ~300 |

**Approach**: Wrote inline Python automation script (piped through PowerShell).
Always picked `legal_moves[0]` (first legal move). Used lower token reporting
(200 prompt + 100 completion per move).

**Baseline**: 6 draws, 2 losses, 2 wins(?). Two losses (score=0.0) pulled
baseline ELO down to 692. Drawing at eval against ELO 1204 jumped final to
1106 — the +414 gain is mostly a measurement artifact from high RD (266).

**Issues found**:
- First attempt blocked by sandbox (read-only). Required `--full-auto` flag.
- PowerShell escaping for inline Python was gnarly but worked.

---

### OpenCode v1.1.53 (gpt-5-nano, free tier)
| Metric | Value |
|--------|-------|
| Completed | No |
| Moves Played | 6 |
| Tokens Used | 35,000 |

**Approach**: Made individual CLI calls per move. Played 6 baseline moves
then stopped to ask for user confirmation — did not loop autonomously.

**Issues found**:
- `opencode run` is single-turn: it exits after one model response.
- Needs `--continue` flag or wrapper script for autonomous multi-turn play.
- gpt-5-nano chose reasonable moves but the harness doesn't support agentic looping.

## Harness Issues Identified

1. **Token self-reporting is unreliable**: Agents report arbitrary token values
   (300–3500 per move). Need either:
   - External token tracking (API-level, like run_benchmark.py does)
   - Or accept self-reporting with guidance on reasonable values

2. **Practice game bug**: "No legal moves available" on game_001 after starting
   a practice game. Needs investigation in session.py / game_manager.

3. **GM game content empty**: Study command returned 5 games but the agent's
   output showed empty game content. The DCN data may not be rendering in the
   JSON response.

4. **Mostly draws**: With MCTSLite opponents, nearly all games end at the
   100-turn draw limit. Need trained NN opponents for decisive games.

5. **Single eval game**: Budget exhaustion limits eval to 1 game (RD stays
   very high at 250+), making ELO gain unreliable.

6. **OpenCode not autonomous**: `opencode run` is single-turn. Would need a
   wrapper loop or the `--continue` flag for benchmark use.

## Harness Lifecycle Validation

| Phase Transition | Status |
|-----------------|--------|
| BASELINE start | Works |
| BASELINE play moves | Works |
| BASELINE -> LEARNING (games done) | Works (Codex) |
| BASELINE -> LEARNING (budget exhausted) | Works (Claude) |
| LEARNING: study GM games | Works |
| LEARNING: practice games | Partial (bug on game_001) |
| LEARNING -> EVALUATION | Works |
| EVALUATION play moves | Works |
| EVALUATION -> COMPLETED (budget) | Works |
| result command | Works |

## Agent Comparison Notes

- **Claude Code** is the most capable harness — autonomous, writes sophisticated
  code, self-corrects errors. But over-reports tokens.
- **Codex CLI** requires `--full-auto` for autonomous execution. Uses PowerShell.
  Simpler strategy but better token management.
- **OpenCode** needs work for benchmark use. Single-turn `run` mode doesn't
  support the 100+ CLI calls needed per benchmark run.

## Next Steps
- [ ] Fix practice game "No legal moves" bug
- [ ] Fix GM game content rendering in study output
- [ ] Finish v2 neural network training
- [ ] Redo calibration with trained NN
- [ ] Regenerate GM games from NN self-play
- [ ] Test OpenCode with `--continue` wrapper
- [ ] Test with GPT 5.2 (Codex vs OpenCode, same model comparison)
- [ ] Add Gemini CLI test
