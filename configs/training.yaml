# AlphaZero training configuration
# Tuned for Jetson Orin Nano Super (8GB shared, CUDA 12.6)
network:
  num_res_blocks: 10
  num_filters: 256
  input_planes: 18
  board_size: 8
  value_head_dropout: 0.3          # Prevent value head memorization (Known Issue #27).
                                    # Without dropout, value head memorizes 15K training positions
                                    # (loss 0.1) but hallucinates ±0.9 on unseen positions,
                                    # hijacking MCTS Q-values during self-play.

mcts:
  num_simulations: 128         # 128 sims with ~37 legal moves = ~3.5 visits/move.
  min_selfplay_simulations: 128 # Match base sims — adaptive scaling starts from here
  # fixed_selfplay_simulations: 64  # Uncomment to disable adaptive sims entirely
  cpuct: 4.0                   # High cpuct trusts policy prior over noisy Q-values.
                                # At 2.5, self-play games were 100% draws (repetition/turn-limit)
                                # because low exploration led to cyclic play. 4.0 produces decisive
                                # games needed for value head learning signal.
  value_scale: 0.5             # Scale NN value predictions before MCTS backprop.
                                # Raw value head hallucinates ±0.9 on unseen positions;
                                # scaling to ±0.45 keeps Q within exploration term range.
  dirichlet_alpha: 0.15            # Spikier noise for fewer legal moves than Go
  dirichlet_epsilon: 0.4            # 40% noise to break self-play overfitting
  temperature_threshold: 30    # Exploit in middlegame/endgame where tactics matter

gumbel:
  enabled: true                    # RE-ENABLED: with value head dropout + value_scale,
                                   # the death spiral (Issue #25) is mitigated. Gumbel produces
                                   # better policy targets from fewer sims. Issues #23/#24 fixed.
  max_num_considered_actions: 16   # Top-k actions for Sequential Halving
  gumbel_scale: 1.0               # Scale of Gumbel noise
  maxvisit_init: 50.0             # Q-transform: baseline visit scale
  value_scale: 0.1                # Q-transform: Q-value scaling factor

selfplay:
  num_games_per_iteration: 40  # More games = more diverse data, less overfitting per position
  replay_buffer_size: 20000    # Total (informational only, actual = seed + decisive + draw)
  buffer_seed_size: 12000      # Permanent seed partition (checkmates, tactical wins)
  buffer_decisive_size: 10000  # Circular: self-play decisive positions — ~25% turnover/iter
  buffer_draw_size: 5000       # Circular: self-play draw positions — ~25% turnover/iter
  num_workers: 4               # CPU worker processes for MCTS tree traversal
  min_buffer_size: 1000        # Train immediately on seed games
  random_opponent_fraction: 0.25  # 25% vs random MCTS — enough diversity without wasting data
  parallel_games: 20             # Fallback for single-process mode (multiprocess ignores this)
  draw_value_target: 0.0       # Neutral — -0.1 caused systematic pessimism (75%+ of data is draws)
  policy_target_smoothing: 0.0 # Disabled for now.

training:
  optimizer: muon               # muon = Muon for conv/hidden + SGD for heads/biases/BN
  batch_size: 128
  learning_rate: 0.01           # Muon LR (reduced from 0.02 to prevent overfitting)
  muon_momentum: 0.95           # Muon momentum (default)
  head_lr: 0.001                # SGD LR for policy/value heads and biases (reduced from 0.003)
  momentum: 0.9
  weight_decay: 0.0001
  value_loss_weight: 10.0      # Restored to give value head ~14.5% of gradient.
                                # With dropout preventing memorization, the value head needs
                                # enough gradient to learn from decisive game outcomes.
                                # Raw value loss (~0.1) is ~60x smaller than policy loss (~5.9).
  policy_freeze_iterations: 0  # Disabled
  policy_entropy_weight: 0.0   # DISABLED: was actively fighting learning from random init.
                                # With legal-move masking in MCTS, the policy head naturally
                                # concentrates on legal moves. Entropy reg pushed it back to
                                # uniform, counteracting the little signal that existed.
  draw_sample_weight: 0.4      # Downweight draw positions in loss vs decisive outcomes
  steps_per_iteration: 400     # ~2 passes through buffer. Was 200 (1 pass) which was too few
                                # to learn. Was 800 (~5 passes) which overfit without dropout.
                                # Dropout (0.3) now prevents value head memorization.
  checkpoint_interval: 100
  elo_probe_interval: 5        # Every 5 iters — watch for improvement with fixed max_moves
  elo_probe_games: 6           # Keep probes under ~1hr at 800 sims
  elo_probe_mctslite_sims: 50
  elo_probe_nn_sims: 800       # AlphaZero-level search for accurate measurement
  elo_probe_max_moves: 200     # Match self-play turn limit (100 full turns = 200 half-moves).
  adaptive_elo_smoothing: 0.7  # Smooth probe ELO before mapping to adaptive sims
  seed_removal_elo: 650        # Clear seed partition once ELO exceeds this
  seed_sample_weight_init: 0.0  # DISABLED: seeds have 4:1 White win bias (Known Issue #21).
                                # Board flipping makes this worse since the bias becomes
                                # encoded differently. Pure self-play from random init.
  seed_sample_weight_decay: 0.93 # (unused while init=0)
  seed_sample_weight_min: 0.0   # (unused while init=0)
  max_iterations: 200

paths:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  training_log: "training_log.jsonl"
